#!/bin/bash

export ENV_INIT_SCRIPT_PATH=$HOME/Documents/Github/sources/public/DevOps/provisioning/production

source $ENV_INIT_SCRIPT_PATH/env_init.sh

echo "checking if ssh key pair exists or not..."

if  [ -r $AWS_SSH_KEYPATH ] && [ -r $AWS_SSH_KEYPATH.pub ]; then
	echo "ssh key pair exists, will use environment variable for AWS_SSH_KEYPATH, moving forward"
else
	echo "ssh key pair does not exist, hence will unset AWS_SSH_KEYPATH"
	echo "now ssh key pair will be generated by docker-machine itself"
	unset AWS_SSH_KEYPATH
	echo "AWS_SSH_KEYPATH unset, checking the value if still exists or not value=$AWS_SSH_KEYPATH"
fi


AWS_ZONE_MANAGER=('a' 'b' 'a' 'b' 'a' 'b' 'a' 'b')
AWS_ZONE_WORKER=('a' 'b' 'a' 'b' 'a' 'b' 'a' 'b' 'a' 'b' 'a' 'b' 'a' 'b' 'a' 'b' 'a')
AWS_INSTANCE_TYPES_MANAGER=('t2.micro' 't2.micro' 't2.micro' 't2.micro' 't2.micro' 't2.micro' 't2.micro' 't2.micro')
AWS_INSTANCE_TYPES_WORKER=('t2.micro' 't2.micro' 't2.micro' 't2.micro' 't2.micro' 't2.micro' 't2.micro' 't2.micro' 't2.micro' 't2.micro' 't2.micro' 't2.micro' 't2.micro' 't2.micro' 't2.micro' 't2.micro' 't2.micro')
AWS_ROOT_SIZES_MANAGER=(8 8 8 8 8 8 8 8)
AWS_ROOT_SIZES_WORKER=(8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8)


# export CLUSTER_SIZE=5
# export MANAGER_COUNT=3
# export WORKER_COUNT=5
# export MACHINE_DRIVER=amazonec2
# export AWS_ACCESS_KEY_ID=<access_key_id>
# export AWS_SECRET_ACCESS_KEY=<secret_access_key>
# export AWS_DEFAULT_REGION=ap-south-1
# export AWS_AMI=ami-dd3442b2
# export AWS_VPC_ID=<aws_vpc_id>
# export AWS_SECURITY_GROUP=docker-machine
# export AWS_VOLUME_TYPE=gp2
# export AWS_SSH_USER=ubuntu
# export AWS_SSH_KEYPATH=<path/to/custom/ssh/private/key>
# export CREATE_SWARM=yes

# --amazonec2-subnet-id	AWS_SUBNET_ID	-
# --amazonec2-device-name	AWS_DEVICE_NAME	/dev/sda1
# --amazonec2-iam-instance-profile	AWS_INSTANCE_PROFILE	-
# --amazonec2-request-spot-instance	-	false
# --amazonec2-spot-price	-	0.50
# --amazonec2-use-private-address	-	false
# --amazonec2-private-address-only	-	false
# --amazonec2-monitoring	-	false
# --amazonec2-use-ebs-optimized-instance	-	false
# --amazonec2-retries	-	5



# export CREATE_STATEMENT="docker-machine create "
export REMOVE="docker-machine rm --force -y "
export START="docker-machine start "
export STOP="docker-machine stop "
# export IP="docker-machine ip "


function create_node() {
   
   local CREATE="docker-machine create \
   --amazonec2-zone $1 \
   --amazonec2-tags $2 \
   --amazonec2-instance-type $3 \
   --amazonec2-root-size $4 \
   $5"
   
   # echo "CREATE : $CREATE"
   # echo "creating node..."
   $CREATE > /dev/null 2>&1
   # echo "creaded now"
   # # sleep 2
   
   # while [ $? -ne 0 ]; do
   #      $REMOVE $5 > /dev/null 2>&1
   #      # sleep 2
   #      $CREATE 2> /dev/null
   #      # sleep 2
   # done
}



function start_node() {
	$START $1 2> /dev/null
	# # sleep 2
	while [ $? -ne 0 ]; do
		$STOP $1 > /dev/null 2>&1
		# sleep 2
		$START $1 2> /dev/null
		# sleep 2
	done
}




function aws_ec2_authorize_security_group_ingress_swarm() {
  
  aws ec2 authorize-security-group-ingress \
  --group-name "$AWS_SECURITY_GROUP" \
  --source-group "$AWS_SECURITY_GROUP" \
  --protocol $1 \
  --port $2  > /dev/null 2>&1 || true

}


function aws_ec2_authorize_security_group_ingress_internet() {
  
  aws ec2 authorize-security-group-ingress \
  --group-name "$AWS_SECURITY_GROUP" \
  --protocol $1 \
  --port $2 \
  --cidr $3 > /dev/null 2>&1 || true

}



function change_docker_env_to() {

	eval $(docker-machine env $1)
}



function inspect_docker_node() {

	docker node inspect $1 > /dev/null 2>&1
}




manager_index=0
worker_index=0

# create a docker swarm cluster of 3 nodes - 1 master and  2 worker
for i in $(seq 0 $((CLUSTER_SIZE-1)));
do
	NODE_TYPE="Worker"
	
	if [ $i -lt $MANAGER_COUNT ];
	then
		NODE_TYPE="Manager"
		CLUSTER_NODE_NAME=${CLUSTER_MANAGER_NAMES[$manager_index]}-0$((i+1))
		AWS_ZONE=${AWS_ZONE_MANAGER[$manager_index]}
		AWS_TAGS="Name,${CLUSTER_MANAGER_NAMES[$manager_index]}-0$((i+1))"
		AWS_INSTANCE_TYPE=${AWS_INSTANCE_TYPES_MANAGER[$manager_index]}
		AWS_ROOT_SIZE=${AWS_ROOT_SIZES_MANAGER[$manager_index]}
		manager_index=$((manager_index + 1))
	else
		NODE_TYPE="Worker"
		CLUSTER_NODE_NAME=${CLUSTER_WORKER_NAMES[$worker_index]}-0$((i+1))
		AWS_ZONE=${AWS_ZONE_WORKER[$worker_index]}
		AWS_TAGS="Name,${CLUSTER_WORKER_NAMES[$manager_index]}-0$((i+1))"
		AWS_INSTANCE_TYPE=${AWS_INSTANCE_TYPES_WORKER[$worker_index]}
		AWS_ROOT_SIZE=${AWS_ROOT_SIZES_WORKER[$worker_index]}
		worker_index=$((worker_index + 1))
	fi

	# create the the swarm nodes
	echo "Checking if Swarm ${NODE_TYPE} Node - $CLUSTER_NODE_NAME exists or not..."
	docker-machine ls -q | grep -w "$CLUSTER_NODE_NAME" > /dev/null 2>&1
	if [ $? -ne 0 ];
	then
		# create the swarm node
		echo "creating Swarm ${NODE_TYPE} Node - $CLUSTER_NODE_NAME..."
		create_node  "$AWS_ZONE"  "$AWS_TAGS"  "$AWS_INSTANCE_TYPE"  "$AWS_ROOT_SIZE"  "$CLUSTER_NODE_NAME" 
		echo "Swarm ${NODE_TYPE} Node created: $CLUSTER_NODE_NAME"

	else
		echo "Swarm ${NODE_TYPE} Node - $CLUSTER_NODE_NAME  already exists"
		echo "checking machine status, start if currently stopped, otherwise move forward"
		docker-machine status "$CLUSTER_NODE_NAME" | grep -w "Stopped" > /dev/null 2>&1
		if [ $? -eq 0 ];
		then
			# start the stopped cluster node machine
			echo "$CLUSTER_NODE_NAME machine is Stopped, hence starting..."
			start_node "$CLUSTER_NODE_NAME"
			echo "$CLUSTER_NODE_NAME machine started Successfully"
		else
			echo "$CLUSTER_NODE_NAME machine is already running, moving forward"
		fi
	fi

done


# list the cluster machines
echo "Current Docker Hosts:"
docker-machine ls


# update the security group for ssl traffic
echo "Updating security group $AWS_SECURITY_GROUP for SSL Internet"
aws_ec2_authorize_security_group_ingress_internet tcp 443 0.0.0.0/0
aws_ec2_authorize_security_group_ingress_internet tcp 80 0.0.0.0/0

echo "Checking if swarm to be created or not..."
# proceed with swarm creation only if necesssary
if [ "$CREATE_SWARM" == "yes" ]; then

	echo "Swarm Creation Starting..."
	echo "Updating security group $AWS_SECURITY_GROUP for swarm"
	##############
	### NOTE: ###
	##############
	# The following ports must be available. On some systems, these ports are open by default.
	# 
	# TCP port 2377 for cluster management communications
	# TCP and UDP port 7946 for communication among nodes
	# TCP and UDP port 4789 for overlay network traffic
	# 
	# If you are planning on creating an overlay network with encryption (--opt encrypted), 
	# you will also need to ensure ip protocol 50 (ESP) traffic is allowed.


	################################################################################################
	# update the security group docker-machine or whatever is given 
	# to open connections for docker swam since the docker-machine as of now (January, 2017)
	# is unable to create the necessary security group rules automatically
	aws_ec2_authorize_security_group_ingress_swarm tcp 7946 
	aws_ec2_authorize_security_group_ingress_swarm udp 7946
	aws_ec2_authorize_security_group_ingress_swarm tcp 4789
	aws_ec2_authorize_security_group_ingress_swarm udp 4789

	echo "Security Group Updated Successfully"

	MAIN_SWARM_MANAGER=${CLUSTER_MANAGER_NAMES[0]}-01
	echo "MAIN_SWARM_MANAGER : $MAIN_SWARM_MANAGER"
	MAIN_SWARM_MANAGER_IP=$(docker-machine ip  "$MAIN_SWARM_MANAGER")
	echo "Main Swarm Manager IP : $MAIN_SWARM_MANAGER_IP"

	# change docker machine env to main swarm manager
	change_docker_env_to "$MAIN_SWARM_MANAGER"

	# check active machine status again
	echo "Active Machine : $(docker-machine active)"

	# init swarm (need for service command); if not created
	echo "Checking if Swarm is already initialized..."
	docker node ls > /dev/null 2>&1 | grep "Leader"
	if [ $? -ne 0 ]; 
	then
		# initialize swarm mode
		echo "Swarm not initialzed, hence starting..."
		echo "Initializing Swarm..."
		docker swarm init --advertise-addr "$MAIN_SWARM_MANAGER_IP" > /dev/null 2>&1
		echo "Swarm Initialized"
	else
		echo "Swarm already initailized, moving forward"
	fi


	# save the swarm token to use in the rest of the nodes
	SWARM_WORKER_JOIN_TOKEN=$(docker swarm join-token -q worker)
	SWARM_MANAGER_JOIN_TOKEN=$(docker swarm join-token -q manager)

	# initialize the managers to join the swarm
	# but, before that check if the node has already joined the swarm as manager or not
	manager_index=1
	worker_index=0

	echo "Checking if the rest of the nodes are initialized, if yes, move forward, else join the swarm"
	for i in $(seq 1 $((CLUSTER_SIZE-1)));
	do

		if [ $i -lt $MANAGER_COUNT ];
		then
			CLUSTER_NODE_NAME=${CLUSTER_MANAGER_NAMES[$manager_index]}-0$((i+1))
			SWARM_JOIN_TOKEN=$SWARM_MANAGER_JOIN_TOKEN
			manager_index=$((manager_index + 1))
		else
			CLUSTER_NODE_NAME=${CLUSTER_WORKER_NAMES[$worker_index]}-0$((i+1))
			SWARM_JOIN_TOKEN=$SWARM_WORKER_JOIN_TOKEN
			worker_index=$((worker_index + 1))
		fi

		inspect_docker_node "$CLUSTER_NODE_NAME"
	    if [ $? -ne 0 ]; 
		then
			echo "$CLUSTER_NODE_NAME node have not joined $MAIN_SWARM_MANAGER manager"
			echo "$CLUSTER_NODE_NAME node joining $MAIN_SWARM_MANAGER manager"
			change_docker_env_to "$MAIN_SWARM_MANAGER"
			echo "Active Machine : $(docker-machine active)"
			echo "$CLUSTER_NODE_NAME node joining swarm mananger $MAIN_SWARM_MANAGER..."
			# first leave any previous swarm if at all
			docker swarm leave  > /dev/null 2>&1
			docker swarm join --token  "$SWARM_JOIN_TOKEN"  "$MANAGER_IP":2377
			echo "$CLUSTER_NODE_NAME joined swarm managed by $MAIN_SWARM_MANAGER"
		else
			echo "$CLUSTER_NODE_NAME node already joined $MAIN_SWARM_MANAGER manager, moving forward"
		fi

		change_docker_env_to "$CLUSTER_NODE_NAME"
		echo "Active Machine : $(docker-machine active)"

	done

	echo "Swarm Nodes Initialization completed"
	echo "Current Swarm Nodes:"
	docker node ls

else 

	 echo "Swarm Creation Not Required, moving forward"
fi



manager_index=0
worker_index=0

echo "Adding user ubuntu to docker group and Installing docker-compose in each node in the swarm..."
# install docker-compose in each node
for i in $(seq 0 $((CLUSTER_SIZE-1)));
do

	if [ $i -lt $MANAGER_COUNT ];
	then
		CLUSTER_NODE_NAME=${CLUSTER_MANAGER_NAMES[$manager_index]}-0$((i+1))
		manager_index=$((manager_index + 1))
	else
		CLUSTER_NODE_NAME=${CLUSTER_WORKER_NAMES[$worker_index]}-0$((i+1))
		worker_index=$((worker_index + 1))
	fi

	echo "processing for swarm node : $CLUSTER_NODE_NAME"
	docker-machine ssh "$CLUSTER_NODE_NAME" '
	if ! which docker-compose > /dev/null 2>&1; then 
		echo "installing docker-compose..."
		sudo curl -L "https://github.com/docker/compose/releases/download/1.10.0/docker-compose-$(uname -s)-$(uname -m)" \
		-o /usr/local/bin/docker-compose; 
		sudo chmod +x /usr/local/bin/docker-compose; 
	else
		echo "docker-compose already installed, moving forward"
	fi
	echo "adding user ubuntu to group docker"
	sudo usermod -aG docker ubuntu > /dev/null 2>&1
	if [ $? -ne 0 ]; then 
		echo "user ubuntu unable to be added to group docker"
	else
		echo "user ubuntu added to group docker successfully"
	fi
	'
	
	echo "processing done for $CLUSTER_NODE_NAME"

done

echo "User ubuntu addded to group docker for all nodes"
echo "docker-compose installed successfully in all nodes"



# manager_index=0
# worker_index=0

# echo "Dns management for all the nodes im the swarm"
# # add dns nameservers pointing to google nameservers in /etc/resolv.conf due to a bug/error
# # whcih does not allow to pull images from docker registry (using docker version 1.13.0)
# for i in $(seq 0 $((CLUSTER_SIZE-1)));
# do

# 	if [ $i -lt $MANAGER_COUNT ];
# 	then
# 		CLUSTER_NODE_NAME=${CLUSTER_MANAGER_NAMES[$manager_index]}-0$((i+1))
# 		manager_index=$((manager_index + 1))
# 	else
# 		CLUSTER_NODE_NAME=${CLUSTER_WORKER_NAMES[$worker_index]}-0$((i+1))
# 		worker_index=$((worker_index + 1))
# 	fi

# 	echo "adding dns entry to /etc/resolv.conf for swarm node : $CLUSTER_NODE_NAME"
# 	docker-machine ssh "$CLUSTER_NODE_NAME" \
# 	'echo -e "nameserver 8.8.8.8\nnameserver 8.8.4.4" | sudo  cat - /etc/resolv.conf > /tmp/out_etc_resolv \
# 	&&  sudo mv /tmp/out_etc_resolv  /etc/resolv.conf \
# 	&& sudo rm -f /tmp/out_etc_resolv >/dev/null 2>&1 &'
	
# 	echo "dns entry added successfully for $CLUSTER_NODE_NAME"

# done



echo "Provisioning Successful"

# trap time EXIT

